{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23b0f686-0940-447a-950f-a2beb1990eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values introduced: 49430\n",
      "Total missing values remaining: 0\n",
      "\n",
      "Snippet of imputed data (first 5 rows):\n",
      "      MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup   Latitude  \\\n",
      "0  8.325200      41.0  6.984127   1.023810   538.42184 -1.830732  37.354704   \n",
      "1  8.301400      21.0  6.871610   0.971880  2401.00000 -0.715959  34.936508   \n",
      "2  7.257400      52.0  8.288136   1.302720   496.00000 -0.042194  37.461994   \n",
      "3  5.643100      52.0  6.379317   1.073059   558.00000  0.060120  37.850000   \n",
      "4  4.622656      52.0  6.281853   1.081081   565.00000  2.919561  37.850000   \n",
      "\n",
      "    Longitude  \n",
      "0 -122.230000  \n",
      "1 -119.591920  \n",
      "2 -122.240000  \n",
      "3 -122.352243  \n",
      "4 -122.250000  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "# 1. Load a real dataset\n",
    "X, y = fetch_california_housing(return_X_y=True)\n",
    "X = pd.DataFrame(X, columns=fetch_california_housing().feature_names)\n",
    "\n",
    "# 2. Artificially introduce missing values for the example\n",
    "rng = np.random.RandomState(42)\n",
    "missing_rate = 0.3\n",
    "# Create a mask for missing values (True means missing)\n",
    "mask = rng.rand(*X.shape) < missing_rate\n",
    "X_missing = X.copy()\n",
    "X_missing[mask] = np.nan\n",
    "\n",
    "print(\"Number of missing values introduced:\", X_missing.isnull().sum().sum())\n",
    "\n",
    "# Scale the data before imputation to help convergence\n",
    "scaler = StandardScaler()\n",
    "# Fit and transform the missing data using the scaler\n",
    "X_missing_scaled = scaler.fit_transform(X_missing)\n",
    "# NOTE: We now work with the numpy array X_missing_scaled, not the pandas DataFrame X_missing\n",
    "\n",
    "# 3. Define and apply the Iterative Imputer\n",
    "# The IterativeImputer is still experimental, requiring the explicit import\n",
    "# We increased max_iter to 200 to address the warning\n",
    "imputer = IterativeImputer(random_state=42, max_iter=100)\n",
    "\n",
    "# Apply imputation on the SCALED data\n",
    "X_imputed_scaled = imputer.fit_transform(X_missing_scaled)\n",
    "\n",
    "# Convert the imputed, scaled numpy array back to original scale and into a pandas DataFrame\n",
    "X_imputed_final = scaler.inverse_transform(X_imputed_scaled) # Convert back to original scale\n",
    "X_imputed = pd.DataFrame(X_imputed_final, columns=X.columns)\n",
    "\n",
    "\n",
    "print(\"Total missing values remaining:\", X_imputed.isnull().sum().sum())\n",
    "print(\"\\nSnippet of imputed data (first 5 rows):\\n\", X_imputed.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4e5e2d2-c255-45dd-9cfe-ad2a38986621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values introduced: 181\n",
      "Total missing values remaining: 0\n",
      "\n",
      "Snippet of imputed data (first 5 rows):\n",
      "    sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0           5.100000          3.500000           1.400000          0.200000\n",
      "1           4.897792          3.225044           1.472040          0.200000\n",
      "2           4.700000          3.200000           1.336796          0.200000\n",
      "3           4.600000          3.117292           1.319896          0.196317\n",
      "4           5.000000          3.600000           1.400000          0.308688\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "# 1. Load a real dataset\n",
    "\n",
    "data = load_iris()\n",
    "X = pd.DataFrame(data=data.data, columns=data.feature_names)\n",
    "\n",
    "# 2. Artificially introduce missing values for the example\n",
    "rng = np.random.RandomState(42)\n",
    "missing_rate = 0.3\n",
    "# Create a mask for missing values (True means missing)\n",
    "mask = rng.rand(*X.shape) < missing_rate\n",
    "X_missing = X.copy()\n",
    "X_missing[mask] = np.nan\n",
    "\n",
    "print(\"Number of missing values introduced:\", X_missing.isnull().sum().sum())\n",
    "\n",
    "# Scale the data before imputation to help convergence\n",
    "scaler = StandardScaler()\n",
    "# Fit and transform the missing data using the scaler\n",
    "X_missing_scaled = scaler.fit_transform(X_missing)\n",
    "# NOTE: We now work with the numpy array X_missing_scaled, not the pandas DataFrame X_missing\n",
    "\n",
    "# 3. Define and apply the Iterative Imputer\n",
    "# The IterativeImputer is still experimental, requiring the explicit import\n",
    "# We increased max_iter to 200 to address the warning\n",
    "imputer = IterativeImputer(random_state=42, max_iter=200)\n",
    "\n",
    "# Apply imputation on the SCALED data\n",
    "X_imputed_scaled = imputer.fit_transform(X_missing_scaled)\n",
    "\n",
    "# Convert the imputed, scaled numpy array back to original scale and into a pandas DataFrame\n",
    "X_imputed_final = scaler.inverse_transform(X_imputed_scaled) # Convert back to original scale\n",
    "X_imputed = pd.DataFrame(X_imputed_final, columns=X.columns)\n",
    "\n",
    "\n",
    "print(\"Total missing values remaining:\", X_imputed.isnull().sum().sum())\n",
    "print(\"\\nSnippet of imputed data (first 5 rows):\\n\", X_imputed.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c30f12b3-df38-4675-91cd-a4788a1a7b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall RMSE of imputed values vs true values: 0.3494\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# --- Code to add AFTER the imputation process is complete ---\n",
    "\n",
    "# 1. Isolate the original true values that were removed\n",
    "# 'mask' is the boolean mask that defined where the NaNs were\n",
    "# X (original dataframe) indexed by the mask gives the true values\n",
    "true_values = X[mask]\n",
    "\n",
    "# 2. Isolate the imputed values at those exact same locations\n",
    "# X_imputed (final dataframe) indexed by the mask gives the generated values\n",
    "imputed_values = X_imputed[mask]\n",
    "\n",
    "# 3. Calculate the performance metrics column by column (or aggregate)\n",
    "\n",
    "## Example: Aggregate RMSE across all features\n",
    "# First, flatten the arrays to compare all values as a single list\n",
    "true_flat = true_values.values.flatten()\n",
    "imputed_flat = imputed_values.values.flatten()\n",
    "\n",
    "# Remove any potential remaining NaNs (if any, though in your case it should be 0)\n",
    "# This step is mainly for robustness\n",
    "valid_comparison_mask = ~np.isnan(true_flat) & ~np.isnan(imputed_flat)\n",
    "true_flat = true_flat[valid_comparison_mask]\n",
    "imputed_flat = imputed_flat[valid_comparison_mask]\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(true_flat, imputed_flat))\n",
    "\n",
    "print(f\"\\nOverall RMSE of imputed values vs true values: {rmse:.4f}\")\n",
    "\n",
    "# Example: MAE for a single column, e.g., 'MedInc'\n",
    "col_name = 'MedInc'\n",
    "if col_name in true_values.columns:\n",
    "    mae_col = np.mean(np.abs(true_values[col_name] - imputed_values[col_name]))\n",
    "    print(f\"Mean Absolute Error for '{col_name}': {mae_col:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
